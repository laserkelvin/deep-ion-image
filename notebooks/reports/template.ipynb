{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import peakutils\n",
    "import abel\n",
    "import os\n",
    "from torch.nn import functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "from ruamel.yaml import YAML\n",
    "\n",
    "from dii.models.base import valid_models\n",
    "from dii.visualization.visualize import radial_profile\n",
    "from dii.pipeline.datautils import get_benchmark_imageset\n",
    "\n",
    "plt.style.use(\"publication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_profile_peaks(x: np.ndarray, y: np.ndarray, thres=0.05):\n",
    "    idx = peakutils.indexes(y, thres=thres)\n",
    "    centers = peakutils.interpolate(x, y, ind=idx)\n",
    "    return idx, centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing report\n",
    "\n",
    "This notebook is generated using `papermill`.\n",
    "\n",
    "## Parameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameter cell; do not edit!\n",
    "model_kwargs = {\n",
    "    \"in_channels\": 1,\n",
    "    \"out_channels\": 1,\n",
    "    \"latent_dim\": 64,\n",
    "    \"activation\": \"silu\"\n",
    "}\n",
    "image_index = 50\n",
    "models_path = \"../../models/\"\n",
    "benchmark_path = \"../../data/processed\"\n",
    "n_images = 128\n",
    "model_name = \"baseline\"\n",
    "probabilistic = False\n",
    "img_center = (64, 64)\n",
    "output_root = \"outputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output path is where model specific results go\n",
    "# agg path is a YAML with the combined statistics for summarizing across models\n",
    "output_path = f\"{output_root}{model_name}/\"\n",
    "agg_path = f\"{output_root}combined_statistics.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(output_path)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model determination and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out what model we are using with the mapping\n",
    "model = valid_models.get(model_name, None)\n",
    "\n",
    "if not model:\n",
    "    raise KeyError(f\"{model_name} is not a valid model in the `dii` codebase! Try again!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj = model(**model_kwargs)\n",
    "model_obj.load_state_dict(torch.load(f\"{models_path}{model_name}.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images, target_images = get_benchmark_imageset(benchmark_path, n_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    recon = model_obj(input_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the mean reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_error = F.binary_cross_entropy(recon, target_images).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean reconstruction error: {recon_error:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best and worst images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this calculates the pixelwise loss\n",
    "errors = F.binary_cross_entropy(recon, target_images, reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the highest errors to identify which images are worse\n",
    "img_errors = errors.mean((1, 2, 3))\n",
    "worst_idx = torch.argsort(img_errors)[-5:]\n",
    "best_idx = torch.argsort(img_errors)[:5]\n",
    "\n",
    "worst_error = img_errors.max().item()\n",
    "best_error = img_errors.min().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the worst images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarray = plt.subplots(3, 5, figsize=(4, 3), sharex=True, sharey=True)\n",
    "\n",
    "for row, (target, title) in enumerate(zip([input_images, recon, target_images], [\"Input\", \"Reconstruction\", \"Target\"])):\n",
    "    images = target[worst_idx]\n",
    "    for col, image in enumerate(images):\n",
    "        # get rid of the channel dimension\n",
    "        axarray[row, col].imshow(image.squeeze())\n",
    "        axarray[row, col].set(xticks=[], yticks=[])\n",
    "        for _, spine in axarray[row, col].spines.items():\n",
    "            spine.set_visible(False)\n",
    "    axarray[row, 0].set_ylabel(title, rotation=90.)\n",
    "for col, value in enumerate(img_errors[worst_idx]):\n",
    "    axarray[-1, col].set(title=f\"{value:.2f}\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{output_path}{model_name}_worstimgs.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the best images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarray = plt.subplots(3, 5, figsize=(4, 3), sharex=True, sharey=True)\n",
    "\n",
    "for row, (target, title) in enumerate(zip([input_images, recon, target_images], [\"Input\", \"Reconstruction\", \"Target\"])):\n",
    "    images = target[best_idx]\n",
    "    for col, image in enumerate(images):\n",
    "        # get rid of the channel dimension\n",
    "        axarray[row, col].imshow(image.squeeze())\n",
    "        axarray[row, col].set(xticks=[], yticks=[])\n",
    "        for _, spine in axarray[row, col].spines.items():\n",
    "            spine.set_visible(False)\n",
    "    axarray[row, 0].set_ylabel(title, rotation=90.)\n",
    "for col, value in enumerate(img_errors[best_idx]):\n",
    "    axarray[-1, col].set(title=f\"{value:.2f}\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{output_path}{model_name}_bestimgs.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common ground: compare one image across models\n",
    "\n",
    "### Show what the reconstruction looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarray = plt.subplots(1, 3, figsize=(2.5, 1), sharex=True, sharey=True)\n",
    "\n",
    "for ax, title, target in zip(axarray, [\"Input\", \"Reconstruction\", \"Target\"], [input_images, recon, target_images]):\n",
    "    ax.imshow(target[image_index].squeeze())\n",
    "    ax.set(title=title, yticks=[], xticks=[])\n",
    "    for _, spine in ax.spines.items():\n",
    "        spine.set_visible(False)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{output_path}{model_name}_common_imgs.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare radial profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if probabilistic:\n",
    "    # get 200 samples from the posterior\n",
    "    prob_recon = model_obj.predict(input_images[image_index], 200)\n",
    "    prob_profiles = np.vstack([radial_profile(img.squeeze().numpy(), img_center) for img in prob_recon])\n",
    "    # calculate sampling statistics\n",
    "    profile_mean = prob_profiles.mean(axis=0)\n",
    "    normalize = profile_mean.sum()\n",
    "    profile_std = prob_profiles.std(axis=0)\n",
    "    # get +/- one sigma and the mean\n",
    "    upper, lower = (profile_mean + profile_std) / normalize, (profile_mean - profile_std) / normalize\n",
    "    profile_mean /= normalize\n",
    "    df = pd.DataFrame(data=list(zip(profile_mean, upper, lower)), columns=[\"Model\", \"Upper\", \"Lower\"])\n",
    "else:\n",
    "    profile = model_obj.model_radial_profile(input_images[image_index])\n",
    "    profile /= profile.sum()\n",
    "    df = pd.DataFrame(data=profile, columns=[\"Model\"])\n",
    "    \n",
    "for target, name in zip([input_images, target_images], [\"Input\", \"Target\"]):\n",
    "    profile = radial_profile(target[image_index].squeeze().numpy(), img_center)\n",
    "    norm = profile.sum()\n",
    "    profile /= norm\n",
    "    df[name] = profile\n",
    "\n",
    "op_img = abel.Transform(input_images[image_index].squeeze().numpy(), method=\"onion_peeling\").transform\n",
    "op_profile = radial_profile(op_img, img_center)\n",
    "op_profile /= op_profile.sum()\n",
    "df[\"OnionPeeling\"] = op_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the result for overlaying later\n",
    "df.to_csv(f\"{output_path}{model_name}_common_radial.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(2.5, 1.5))\n",
    "\n",
    "colors = [\"#0051ad\", \"#d34e60\", \"#9ac54f\"]\n",
    "\n",
    "for key, color in zip([\"Model\", \"Target\", \"OnionPeeling\"], colors):\n",
    "    ax.plot(df[key], lw=1., alpha=0.7, label=key, color=color)\n",
    "\n",
    "# if we have a probabilistic model, shade in +/-1 sigma\n",
    "if probabilistic:\n",
    "    ax.fill_between(np.arange(df[\"Model\"].size), df[\"Upper\"], df[\"Lower\"], color=colors[0], alpha=0.4)\n",
    "    \n",
    "ax.set(ylabel=\"$p(r)$\", xlabel=\"Radial distance ($r$ / pixels)\", xlim=[0., img_center[0]])\n",
    "ax.legend(fontsize=\"xx-small\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{output_path}{model_name}_common_radial.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial error quantification\n",
    "\n",
    "__For the sole exemplar image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_quant, true_interp = get_profile_peaks(df.index, df[\"Target\"])\n",
    "# recon_quant, recon_interp = get_profile_peaks(df.index, df[\"Model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quant_error = (np.square(true_quant - recon_quant)).mean()\n",
    "# interp_error = ((np.square(true_interp - recon_interp)).mean() / df.index.size) * 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Quantized pixel error is {quant_error:.2f} pixels\")\n",
    "# print(f\"Interpolated relative error is {interp_error:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # see what the centers actually are\n",
    "# print(true_interp, recon_interp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Across the benchmark set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_num, pred_num = list(), list()\n",
    "kl_divs = list()\n",
    "for target, predicted in zip(target_images, recon):\n",
    "    target_profile = radial_profile(target.squeeze().numpy(), img_center)\n",
    "    recon_profile = radial_profile(predicted.squeeze().numpy(), img_center)\n",
    "    kl = entropy(pk=target_profile, qk=recon_profile)\n",
    "    kl_divs.append(kl)\n",
    "    # get the peaks from the radial profile\n",
    "    x = np.arange(target_profile.size)\n",
    "    true_quant, true_interp = get_profile_peaks(x, target_profile)\n",
    "    recon_quant, recon_interp = get_profile_peaks(x, recon_profile)\n",
    "    true_num.append(len(true_quant))\n",
    "    pred_num.append(len(recon_quant))\n",
    "\n",
    "true_counts, bins = np.histogram(true_num, bins=np.arange(1, 6))\n",
    "pred_counts, bins = np.histogram(pred_num, bins=np.arange(1, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean KL-divergence, representing the true distribution P with the predicted Q: {np.mean(kl_divs):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(figsize=(2.5, 1.5))\n",
    "\n",
    "ax.bar(bins[:-1] - 0.1, true_counts, label=\"True\", alpha=0.7, width=0.2)\n",
    "ax.bar(bins[:-1] + 0.1, pred_counts, label=\"Model\", alpha=0.7, width=0.2)\n",
    "ax.legend(fontsize=\"x-small\")\n",
    "ax.set(ylabel=\"Counts\", xlabel=\"Number of centers detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml = YAML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = None\n",
    "if os.path.isfile(agg_path):\n",
    "    with open(agg_path, \"r\") as results_file:\n",
    "        current = yaml.load(results_file)\n",
    "if not current:\n",
    "    current = dict()\n",
    "current[f\"{model_name}\"] = {\n",
    "    \"kl-divergence\": str(np.mean(kl_divs)),\n",
    "    \"dev_mean_recon\": str(recon_error),\n",
    "    \"best_recon\": str(best_error),\n",
    "    \"worst_recon\": str(worst_error)\n",
    "}\n",
    "with open(agg_path, \"w\") as results_file:\n",
    "    yaml.dump(current, results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DII)",
   "language": "python",
   "name": "ion-image"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
